{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is enabled with initial size: 75.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Flatten, Embedding, LSTM, merge, TimeDistributed, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/cinc/data/quora-question-pairs/'\n",
    "save_path = '/cinc/data/quora-question-pairs/save_data/'\n",
    "model_path = '/cinc/data/quora-question-pairs/model/'\n",
    "\n",
    "word2vec_file = '/cinc/data/word2vec/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "glove_file = '/cinc/data/glove/glove.840B.300d.txt'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sequence_1 = load_array(save_path + 'train_sequence_1')\n",
    "train_sequence_2 = load_array(save_path + 'train_sequence_2')\n",
    "\n",
    "test_sequence_1 = load_array(save_path + 'test_sequence_1')\n",
    "test_sequence_2 = load_array(save_path + 'test_sequence_2')\n",
    "\n",
    "train_labels = load_array(save_path + 'train_labels')\n",
    "\n",
    "word_index = load_array(save_path + 'word_index')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120501"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words = len(word_index) + 1\n",
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404290, 30), (404290, 30), (2345796, 30), (2345796, 30))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequence_1.shape, train_sequence_2.shape, test_sequence_1.shape, test_sequence_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding_matrix = load_array(save_path + 'embedding_matrix')\n",
    "#embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing  100000\n",
      "processing  200000\n",
      "processing  300000\n",
      "processing  400000\n",
      "processing  500000\n",
      "processing  600000\n",
      "processing  700000\n",
      "processing  800000\n",
      "processing  900000\n",
      "processing  1000000\n",
      "processing  1100000\n",
      "processing  1200000\n",
      "processing  1300000\n",
      "processing  1400000\n",
      "processing  1500000\n",
      "processing  1600000\n",
      "processing  1700000\n",
      "processing  1800000\n",
      "processing  1900000\n",
      "processing  2000000\n",
      "processing  2100000\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(glove_file)\n",
    "count = 0\n",
    "for line in f:\n",
    "    count = count+1\n",
    "    if (count % 100000) == 0:\n",
    "        print 'processing ', count\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(save_path + 'embedding_matrix', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120501, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = load_array(save_path + 'embedding_matrix')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build X_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404290, 30), (404290, 30))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_all_1, X_all_2) = (train_sequence_1, train_sequence_2)\n",
    "X_all_1.shape, X_all_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404290,), 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all = np.array(train_labels)\n",
    "y_all.shape, y_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, ...,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_samples = train_sequence_1.shape[0]\n",
    "#nb_samples = 1000\n",
    "msk = np.random.rand(nb_samples) < 0.8\n",
    "msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323814, 30), (323814, 30), (323814,), (80476, 30), (80476, 30), (80476,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1 = X_all_1[msk]\n",
    "X_train_2 = X_all_2[msk]\n",
    "\n",
    "X_valid_1 = X_all_1[~msk]\n",
    "X_valid_2 = X_all_2[~msk]\n",
    "\n",
    "y_train = y_all[msk]\n",
    "y_valid = y_all[~msk]\n",
    "\n",
    "(X_train_1.shape, X_train_1.shape, y_train.shape, X_valid_1.shape, X_valid_2.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assign weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_valid = np.ones(len(y_valid))\n",
    "weight_valid *= 0.472001959\n",
    "weight_valid[y_valid==0] = 1.309028344\n",
    "weight_valid[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight = {1.309028344, 0.472001959}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    embedding_layer = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable = False\n",
    "    )\n",
    "    num_ltsm = 200\n",
    "    lstm_layer = LSTM(num_ltsm, dropout=0.3, recurrent_dropout=0.3)\n",
    "\n",
    "    input_sequence_1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedding_sequence_1 = embedding_layer(input_sequence_1)\n",
    "    x1 = lstm_layer(embedding_sequence_1)\n",
    "\n",
    "    input_sequence_2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedding_sequence_2 = embedding_layer(input_sequence_2)\n",
    "    x2 = lstm_layer(embedding_sequence_2)\n",
    "\n",
    "    merged = concatenate([x1, x2])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "\n",
    "    merged = Dense(256, activation='relu')(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "\n",
    "    pred_layer = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(inputs=[input_sequence_1, input_sequence_2],\n",
    "                  outputs=pred_layer\n",
    "                 )\n",
    "    #model.summary()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(i, model, X_train_1, X_train_2, y_train, X_valid_1, X_valid_2, y_valid):\n",
    "    model_filename = model_path + 'glove_lstm_class_weight_keras2_Dense256_cv_' + str(i) + '.h5'\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
    "        ModelCheckpoint(model_filename, monitor='val_loss', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    class_weight = {1.309028344, 0.472001959}\n",
    "    \n",
    "    weight_valid = np.ones(len(y_valid))\n",
    "    weight_valid *= 0.472001959\n",
    "    weight_valid[y_valid==0] = 1.309028344\n",
    "    #weight_valid[0:10]\n",
    "\n",
    "    model.load_weights(model_path + 'glove_lstm_class_weight_keras2_Dense256.h5')\n",
    "    #model.load_weights(model_filename)\n",
    "    hist = model.fit([X_train_1, X_train_2], y_train, \n",
    "                     validation_data=([X_valid_1, X_valid_2], y_valid, weight_valid),\n",
    "                     class_weight=class_weight,\n",
    "                     batch_size=2048, epochs=200, verbose=2, shuffle=True,\n",
    "                     callbacks=callbacks\n",
    "                    )\n",
    "    \n",
    "    model.load_weights(model_filename)\n",
    "    pred_valid = model.predict([X_valid_1, X_valid_2], batch_size=8192, verbose=2)\n",
    "    #print sum(np.round(pred_valid) == y_valid.reshape(len(y_valid), 1)) * 1.0 / len(y_valid)\n",
    "    score = log_loss(y_valid, pred_valid)\n",
    "    print ('Score log_loss: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, random_state=37, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323431,) (80859,) [ 0  1  3  4  6  7  8  9 10 11] [ 2  5 17 33]\n",
      "(323431,) (80859,) [ 2  3  4  5  6  7  8  9 12 13] [ 0  1 10 11]\n",
      "(323432,) (80858,) [ 0  1  2  3  4  5  7  8  9 10] [ 6 12 18 23]\n",
      "(323433,) (80857,) [ 0  1  2  3  4  5  6  7  8 10] [ 9 20 21 22]\n",
      "(323433,) (80857,) [ 0  1  2  5  6  9 10 11 12 17] [3 4 7 8]\n",
      "5 5\n",
      "(323431,) (80859,) [ 0  1  3  4  6  7  8  9 10 11] [ 2  5 17 33]\n",
      "(323431,) (80859,) [ 2  3  4  5  6  7  8  9 12 13] [ 0  1 10 11]\n",
      "(323432,) (80858,) [ 0  1  2  3  4  5  7  8  9 10] [ 6 12 18 23]\n",
      "(323433,) (80857,) [ 0  1  2  3  4  5  6  7  8 10] [ 9 20 21 22]\n",
      "(323433,) (80857,) [ 0  1  2  5  6  9 10 11 12 17] [3 4 7 8]\n"
     ]
    }
   ],
   "source": [
    "train_splits = []\n",
    "valid_splits = []\n",
    "for train_index, valid_index in skf.split(X_all_1, y_all):\n",
    "    print train_index.shape, valid_index.shape, train_index[0:10], valid_index[0:4]\n",
    "    train_splits.append(train_index)\n",
    "    valid_splits.append(valid_index)\n",
    "\n",
    "print len(train_splits), len(valid_splits)\n",
    "for i in range(n_folds):\n",
    "    train_index = train_splits[i]\n",
    "    valid_index = valid_splits[i]\n",
    "    print train_index.shape, valid_index.shape, train_index[0:10], valid_index[0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (323431,) (80859,) [ 0  1  3  4  6  7  8  9 10 11] [ 2  5 17 33]\n",
      "Train on 323431 samples, validate on 80859 samples\n",
      "Epoch 1/200\n",
      "70s - loss: 0.4742 - acc: 0.7726 - val_loss: 0.3839 - val_acc: 0.7867\n",
      "Epoch 2/200\n",
      "73s - loss: 0.4682 - acc: 0.7759 - val_loss: 0.3846 - val_acc: 0.7864\n",
      "Epoch 3/200\n",
      "74s - loss: 0.4635 - acc: 0.7787 - val_loss: 0.3727 - val_acc: 0.7854\n",
      "Epoch 4/200\n",
      "74s - loss: 0.4591 - acc: 0.7812 - val_loss: 0.3865 - val_acc: 0.7853\n",
      "Epoch 5/200\n",
      "74s - loss: 0.4555 - acc: 0.7836 - val_loss: 0.3986 - val_acc: 0.7861\n",
      "Epoch 6/200\n",
      "74s - loss: 0.4512 - acc: 0.7862 - val_loss: 0.4032 - val_acc: 0.7845\n",
      "Epoch 7/200\n",
      "74s - loss: 0.4466 - acc: 0.7879 - val_loss: 0.3818 - val_acc: 0.7870\n",
      "Epoch 8/200\n",
      "74s - loss: 0.4431 - acc: 0.7914 - val_loss: 0.4026 - val_acc: 0.7851\n",
      "Epoch 9/200\n",
      "74s - loss: 0.4408 - acc: 0.7921 - val_loss: 0.3751 - val_acc: 0.7876\n",
      "('Score log_loss: ', 0.45377006892675636)\n",
      "\n",
      "1 (323431,) (80859,) [ 2  3  4  5  6  7  8  9 12 13] [ 0  1 10 11]\n",
      "Train on 323431 samples, validate on 80859 samples\n",
      "Epoch 1/200\n",
      "72s - loss: 0.4739 - acc: 0.7724 - val_loss: 0.3876 - val_acc: 0.7857\n",
      "Epoch 2/200\n",
      "74s - loss: 0.4688 - acc: 0.7758 - val_loss: 0.3676 - val_acc: 0.7841\n",
      "Epoch 3/200\n",
      "74s - loss: 0.4630 - acc: 0.7791 - val_loss: 0.3940 - val_acc: 0.7845\n",
      "Epoch 4/200\n",
      "74s - loss: 0.4601 - acc: 0.7807 - val_loss: 0.4076 - val_acc: 0.7857\n",
      "Epoch 5/200\n",
      "74s - loss: 0.4556 - acc: 0.7837 - val_loss: 0.3882 - val_acc: 0.7843\n",
      "Epoch 6/200\n",
      "74s - loss: 0.4509 - acc: 0.7859 - val_loss: 0.3770 - val_acc: 0.7833\n",
      "Epoch 7/200\n",
      "74s - loss: 0.4474 - acc: 0.7880 - val_loss: 0.3923 - val_acc: 0.7855\n",
      "Epoch 8/200\n",
      "74s - loss: 0.4442 - acc: 0.7908 - val_loss: 0.3881 - val_acc: 0.7844\n",
      "('Score log_loss: ', 0.45304279354131849)\n",
      "\n",
      "2 (323432,) (80858,) [ 0  1  2  3  4  5  7  8  9 10] [ 6 12 18 23]\n",
      "Train on 323432 samples, validate on 80858 samples\n",
      "Epoch 1/200\n",
      "73s - loss: 0.4738 - acc: 0.7725 - val_loss: 0.3756 - val_acc: 0.7852\n",
      "Epoch 2/200\n",
      "74s - loss: 0.4684 - acc: 0.7758 - val_loss: 0.3878 - val_acc: 0.7842\n",
      "Epoch 3/200\n",
      "74s - loss: 0.4632 - acc: 0.7792 - val_loss: 0.3784 - val_acc: 0.7822\n",
      "Epoch 4/200\n",
      "74s - loss: 0.4592 - acc: 0.7817 - val_loss: 0.4020 - val_acc: 0.7823\n",
      "Epoch 5/200\n",
      "74s - loss: 0.4555 - acc: 0.7830 - val_loss: 0.3748 - val_acc: 0.7845\n",
      "Epoch 6/200\n",
      "74s - loss: 0.4509 - acc: 0.7862 - val_loss: 0.3813 - val_acc: 0.7844\n",
      "Epoch 7/200\n",
      "74s - loss: 0.4477 - acc: 0.7882 - val_loss: 0.3977 - val_acc: 0.7852\n",
      "Epoch 8/200\n",
      "74s - loss: 0.4442 - acc: 0.7901 - val_loss: 0.3960 - val_acc: 0.7834\n",
      "Epoch 9/200\n",
      "74s - loss: 0.4405 - acc: 0.7917 - val_loss: 0.3867 - val_acc: 0.7837\n",
      "Epoch 10/200\n",
      "74s - loss: 0.4365 - acc: 0.7948 - val_loss: 0.3879 - val_acc: 0.7864\n",
      "Epoch 11/200\n",
      "74s - loss: 0.4335 - acc: 0.7960 - val_loss: 0.3926 - val_acc: 0.7857\n",
      "('Score log_loss: ', 0.45552096003751547)\n",
      "\n",
      "3 (323433,) (80857,) [ 0  1  2  3  4  5  6  7  8 10] [ 9 20 21 22]\n",
      "Train on 323433 samples, validate on 80857 samples\n",
      "Epoch 1/200\n",
      "72s - loss: 0.4738 - acc: 0.7727 - val_loss: 0.3778 - val_acc: 0.7866\n",
      "Epoch 2/200\n",
      "74s - loss: 0.4685 - acc: 0.7759 - val_loss: 0.3933 - val_acc: 0.7869\n",
      "Epoch 3/200\n",
      "74s - loss: 0.4634 - acc: 0.7797 - val_loss: 0.3680 - val_acc: 0.7842\n",
      "Epoch 4/200\n",
      "74s - loss: 0.4595 - acc: 0.7820 - val_loss: 0.3821 - val_acc: 0.7854\n",
      "Epoch 5/200\n",
      "74s - loss: 0.4549 - acc: 0.7836 - val_loss: 0.4014 - val_acc: 0.7859\n",
      "Epoch 6/200\n",
      "74s - loss: 0.4509 - acc: 0.7867 - val_loss: 0.3980 - val_acc: 0.7855\n",
      "Epoch 7/200\n",
      "74s - loss: 0.4468 - acc: 0.7882 - val_loss: 0.4085 - val_acc: 0.7833\n",
      "Epoch 8/200\n",
      "74s - loss: 0.4437 - acc: 0.7902 - val_loss: 0.3832 - val_acc: 0.7870\n",
      "Epoch 9/200\n",
      "74s - loss: 0.4404 - acc: 0.7914 - val_loss: 0.3878 - val_acc: 0.7855\n",
      "('Score log_loss: ', 0.45685869230102855)\n",
      "\n",
      "4 (323433,) (80857,) [ 0  1  2  5  6  9 10 11 12 17] [3 4 7 8]\n",
      "Train on 323433 samples, validate on 80857 samples\n",
      "Epoch 1/200\n",
      "73s - loss: 0.4746 - acc: 0.7727 - val_loss: 0.3726 - val_acc: 0.7857\n",
      "Epoch 2/200\n",
      "75s - loss: 0.4685 - acc: 0.7761 - val_loss: 0.3868 - val_acc: 0.7855\n",
      "Epoch 3/200\n",
      "74s - loss: 0.4636 - acc: 0.7792 - val_loss: 0.3820 - val_acc: 0.7861\n",
      "Epoch 4/200\n",
      "75s - loss: 0.4593 - acc: 0.7810 - val_loss: 0.3833 - val_acc: 0.7850\n",
      "Epoch 5/200\n",
      "74s - loss: 0.4552 - acc: 0.7835 - val_loss: 0.3755 - val_acc: 0.7850\n",
      "Epoch 6/200\n",
      "74s - loss: 0.4514 - acc: 0.7855 - val_loss: 0.3858 - val_acc: 0.7856\n",
      "Epoch 7/200\n",
      "74s - loss: 0.4476 - acc: 0.7887 - val_loss: 0.3900 - val_acc: 0.7843\n",
      "('Score log_loss: ', 0.4515739136607661)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "#for i, (train, valid) in enumerate(skf):\n",
    "for i in range(5):\n",
    "    train_index = train_splits[i]\n",
    "    valid_index = valid_splits[i]\n",
    "    print i, train_index.shape, valid_index.shape, train_index[0:10], valid_index[0:4]\n",
    "    model = createModel()\n",
    "    models.append(model)\n",
    "    \n",
    "    trainModel(i, model, X_all_1[train_index], X_all_2[train_index], y_all[train_index], \n",
    "               X_all_1[valid_index], X_all_2[valid_index], y_all[valid_index])\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#models[0].optimizer.lr.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2345796, 30), (2345796, 30))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_test_1, X_test_2) = (test_sequence_1, test_sequence_2)\n",
    "X_test_1.shape, X_test_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [16:14<00:00, 194.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02602234  0.30488801  0.08215218  0.36361808  0.56934977]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(n_folds)):\n",
    "    pred_test_i = models[i].predict([X_test_1, X_test_1], batch_size=8192, verbose=2)\n",
    "    #print pred_test_i[0:5][:,0]\n",
    "    if (i == 0):\n",
    "        pred_test = pred_test_i\n",
    "    else:\n",
    "        pred_test += pred_test_i\n",
    "pred_test = pred_test / 5\n",
    "print pred_test[0:5][:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2345796, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  is_duplicate\n",
       "0        0             1\n",
       "1        1             1\n",
       "2        2             1\n",
       "3        3             1\n",
       "4        4             1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission = pd.read_csv(path + 'submission/sample_submission.csv')\n",
    "print df_submission.shape\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.026022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.304888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.082152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.363618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.569350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  is_duplicate\n",
       "0        0      0.026022\n",
       "1        1      0.304888\n",
       "2        2      0.082152\n",
       "3        3      0.363618\n",
       "4        4      0.569350"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission['is_duplicate'] = pred_test\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission.to_csv(path + 'submission/quora_glove_lstm_class_weight_keras2_cv_attempt2_20170605.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
